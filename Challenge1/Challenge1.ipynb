{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ca7ac3ac-5154-4082-a8a5-426b4a970eff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "SPDX-License-Identifier: Apache-2.0\n",
    "Copyright (c) 2023, Rahul Unnikrishnan Nair <rahul.unnikrishnan.nair@intel.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f57b36-16a0-4908-bc09-e09af13bcaa3",
   "metadata": {},
   "source": [
    "# Challenge 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95841e46-5430-4cb7-9733-de863e2ae321",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "Craft a captivating story told through a sequence of 4-5 images, created using Stable Diffusion text-to-image, image to image models. Your task is to narrate a tale, where each image seamlessly leads to the next, creating a visual narrative. For eg, it could be a comic with 4 images.\n",
    "\n",
    "### Hints:\n",
    "\n",
    "- Story Crafting: Start with a clear storyline. Think of each image as a chapter in your story\n",
    "- Prompt Engineering: For each image, craft detailed prompts. The more specific you are, the better. However, avoid overly complicated prompts as they may lead to confusion. Include descriptors for subject, environment, mood, etc. Checkout different prompting techniques online if you are new to prompt engineering, here is a pdf on prompting for stable diffusion I found online\n",
    "- Sequencing: Ensure each image has a logical or thematic connection to the next, maintaining a consistent narrative flow.\n",
    "- Narrative Coherence: The challenge is to create a coherent visual story, where each image is both a standalone piece and a part of the larger narrative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf1a19-7461-4880-9fab-d54260644c83",
   "metadata": {},
   "source": [
    "Let's get started by setting up few things and importing the required python packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "791c50f9-4524-4410-8ff9-2c55dc674950",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation in progress...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): conda.anaconda.org:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): conda.anaconda.org:443\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/osx-arm64/current_repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/noarch/current_repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/r/noarch/current_repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/r/osx-arm64/current_repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/current_repodata.json HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/current_repodata.json HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): conda.anaconda.org:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): conda.anaconda.org:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/osx-arm64/repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/r/osx-arm64/repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/r/noarch/repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/noarch/repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/repodata.json HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/repodata.json HTTP/1.1\" 200 None\n",
      "WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.8.0.*, but conda is ignoring the .* and treating it as 1.8.0\n",
      "WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.9.0.*, but conda is ignoring the .* and treating it as 1.9.0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): conda.anaconda.org:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): conda.anaconda.org:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): conda.anaconda.org:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): conda.anaconda.org:443\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/osx-arm64/arrow-cpp-11.0.0-hc7aafb3_2.conda HTTP/1.1\" 200 7007112\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/dill-0.3.7-pyhd8ed1ab_0.conda HTTP/1.1\" 200 87553\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/sacremoses-0.0.53-pyhd8ed1ab_0.tar.bz2 HTTP/1.1\" 200 437464\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/libevent-2.1.12-h2757513_1.conda HTTP/1.1\" 200 368167\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/accelerate-0.23.0-pyhd8ed1ab_0.conda HTTP/1.1\" 200 172973\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/diffusers-0.18.2-pyhd8ed1ab_0.conda HTTP/1.1\" 200 372620\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/re2-2022.04.01-h6b3803e_0.tar.bz2 HTTP/1.1\" 200 191435\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/transformers-4.32.1-pyhd8ed1ab_0.conda HTTP/1.1\" 200 2664786\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/osx-arm64/aws-sdk-cpp-1.8.185-ha71a6ea_1.conda HTTP/1.1\" 200 1910799\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/libbrotlienc-1.0.9-h1a8c8d9_9.conda HTTP/1.1\" 200 263314\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/xxhash-0.8.0-h27ca646_3.tar.bz2 HTTP/1.1\" 200 90578\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/libbrotlicommon-1.0.9-h1a8c8d9_9.conda HTTP/1.1\" 200 70285\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/snappy-1.1.10-h17c5cce_0.conda HTTP/1.1\" 200 33879\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/widgetsnbextension-4.0.9-pyhd8ed1ab_0.conda HTTP/1.1\" 200 885957\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/aws-c-common-0.6.8-h3422bc3_0.tar.bz2 HTTP/1.1\" 200 152546\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/osx-arm64/libcurl-8.4.0-h3e2b118_0.conda HTTP/1.1\" 200 363909\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/abseil-cpp-20211102.0-h6b3803e_1.tar.bz2 HTTP/1.1\" 200 1023978\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/aws-checksums-0.1.11-h5267759_12.tar.bz2 HTTP/1.1\" 200 49674\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/jupyterlab_widgets-3.0.9-pyhd8ed1ab_0.conda HTTP/1.1\" 200 186821\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/osx-arm64/libnghttp2-1.57.0-h62f6fdd_0.conda HTTP/1.1\" 200 648894\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/tqdm-4.49.0-pyh9f0ad1d_0.tar.bz2 HTTP/1.1\" 200 55065\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/osx-arm64/libthrift-0.15.0-h73c2103_2.conda HTTP/1.1\" 200 340894\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/gflags-2.2.2-hc88da5d_1004.tar.bz2 HTTP/1.1\" 200 86690\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/brotli-1.0.9-hbdafb3b_4.tar.bz2 HTTP/1.1\" 200 400559\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/dataclasses-0.8-pyhc8e2a94_3.tar.bz2 HTTP/1.1\" 200 9870\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/pytz-2023.3.post1-pyhd8ed1ab_0.conda HTTP/1.1\" 200 187454\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/krb5-1.20.1-h69eda48_0.conda HTTP/1.1\" 200 1094005\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/icu-70.1-h6b3803e_0.tar.bz2 HTTP/1.1\" 200 13900799\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/boost-cpp-1.78.0-hb428463_0.tar.bz2 HTTP/1.1\" 200 17161447\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/libssh2-1.10.0-h7a5bd25_2.tar.bz2 HTTP/1.1\" 200 223984\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/certifi-2023.11.17-pyhd8ed1ab_0.conda HTTP/1.1\" 200 158939\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/python-tzdata-2023.3-pyhd8ed1ab_0.conda HTTP/1.1\" 200 143131\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/osx-arm64/grpc-cpp-1.48.2-hc60591f_1.conda HTTP/1.1\" 200 3164235\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/libbrotlidec-1.0.9-h1a8c8d9_9.conda HTTP/1.1\" 200 29129\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/validators-0.22.0-pyhd8ed1ab_0.conda HTTP/1.1\" 200 25165\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/libev-4.33-h642e427_1.tar.bz2 HTTP/1.1\" 200 100668\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/click-8.1.7-unix_pyh707e725_0.conda HTTP/1.1\" 200 84437\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/libedit-3.1.20191231-hc8eb9b7_2.tar.bz2 HTTP/1.1\" 200 96607\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/datasets-1.9.0-pyhd8ed1ab_1.tar.bz2 HTTP/1.1\" 200 186421\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): conda.anaconda.org:443\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/joblib-1.3.2-pyhd8ed1ab_0.conda HTTP/1.1\" 200 221200\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/huggingface_hub-0.19.4-pyhd8ed1ab_0.conda HTTP/1.1\" 200 205913\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/yaml-0.2.5-h3422bc3_2.tar.bz2 HTTP/1.1\" 200 88016\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/osx-arm64/multiprocess-0.70.15-py311hca03da5_0.conda HTTP/1.1\" 200 356781\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/noarch/ipywidgets-8.1.1-pyhd8ed1ab_0.conda HTTP/1.1\" 200 114155\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/glog-0.5.0-h5c6a83d_0.tar.bz2 HTTP/1.1\" 200 94823\n",
      "DEBUG:urllib3.connectionpool:https://conda.anaconda.org:443 \"GET /conda-forge/osx-arm64/c-ares-1.23.0-h93a5062_0.conda HTTP/1.1\" 200 136858\n",
      "WARNING conda.core.path_actions:verify(1051): Unable to create environments file. Path not writable.\n",
      "  environment location: /Users/tzvifriedman/.conda/environments.txt\n",
      "\n",
      "WARNING conda.core.envs_manager:register_env(52): Unable to register environment. Path not writable or missing.\n",
      "  environment location: /Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/.conda\n",
      "  registry file: /Users/tzvifriedman/.conda/environments.txt\n",
      "Installation successful\n",
      "Collecting package metadata (current_repodata.json): ...working... DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/r/osx-arm64/current_repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/noarch/current_repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/r/noarch/current_repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/osx-arm64/current_repodata.json HTTP/1.1\" 304 0\n",
      "done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/.conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - pillow\n",
      "    - pytorch\n",
      "    - requests\n",
      "\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2023.11.~ --> pkgs/main::ca-certificates-2023.08.22-hca03da5_0 \n",
      "  certifi            conda-forge/noarch::certifi-2023.11.1~ --> pkgs/main/osx-arm64::certifi-2023.11.17-py311hca03da5_0 \n",
      "\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... WARNING conda.core.path_actions:verify(1051): Unable to create environments file. Path not writable.\n",
      "  environment location: /Users/tzvifriedman/.conda/environments.txt\n",
      "\n",
      "done\n",
      "Executing transaction: ...working... WARNING conda.core.envs_manager:register_env(52): Unable to register environment. Path not writable or missing.\n",
      "  environment location: /Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/.conda\n",
      "  registry file: /Users/tzvifriedman/.conda/environments.txt\n",
      "done\n",
      "Installation complete...\n"
     ]
    }
   ],
   "source": [
    "# Required packages, install if not installed (assume PyTorch* and IntelÂ® Extension for PyTorch* is already present)\n",
    "# #import sys\n",
    "!echo \"Installation in progress...\"\n",
    "# for local development uncomment next line and comment the one after\n",
    "# !conda install -y --quiet  --prefix {sys.prefix} -c conda-forge \\\n",
    "#     accelerate==0.23.0 \\\n",
    "#     validators==0.22.0 \\\n",
    "#     diffusers==0.18.2 \\\n",
    "#     transformers==4.32.1 \\\n",
    "#     pillow \\\n",
    "#     ipywidgets \\\n",
    "#     ipython > /dev/null && echo \"Installation successful\" || echo \"Installation failed\"\n",
    "# !python -m pip install intel_extension_for_pytorch -f https://developer.intel.com/ipex-whl-stable-cpu\n",
    "\n",
    "import sys\n",
    "!conda install -y --quiet  --prefix {sys.prefix} invisible-watermark --user > /dev/null 2>&1\n",
    "!conda install -y --quiet  --prefix {sys.prefix} transformers huggingface-hub --user > /dev/null 2>&1\n",
    "# !conda install -y --quiet  --prefix {sys.prefix} requests pytorch \n",
    "\n",
    "!echo \"Installation complete...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee25d6c0-1c17-4497-bd74-f7795887d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Suppress warnings for a cleaner output.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import intel_extension_for_pytorch as ipex  # Used for optimizing PyTorch models\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22619ef2-118d-41b0-ac46-9c80261cc138",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A Glimpse Behind the Scenes**\n",
    "\n",
    "For those intrigued by the underpinnings of this adventure, let's delve into the technicalities of the code. No worries if you're not aiming for a deep dive; understanding this isn't a prerequisite to run the notebook. But for the tech-curious, let's dissect:\n",
    "\n",
    "- **Class**: At the heart of our operations is the `Text2ImgModel` class. This class defines the process of transforming textual prompts into visual masterpieces.\n",
    "\n",
    "- **Pipeline Loading**: The `_load_pipeline` function is where we bring the pre-trained model onboard. This sets the stage for all our text-to-image transformations.\n",
    "\n",
    "- **Optimization**: Performance is paramount. With the `_optimize_pipeline` and `optimize_pipeline` methods, we leverage Intel-specific optimizations using the Intel Extension For PyTorch (IPEX) to ensure our model runs fast!.\n",
    "\n",
    "- **The Grand Finale - Image Generation**: The `generate_images` method is where dreams meet reality. It interprets the textual prompts, consults with the model, and then crafts images that encapsulate the essence of the prompts. You can choose the model, provide a prompt and specify the number of images.\n",
    "\n",
    "Intrigued? Dive into the code below and see how we've tailored the image generation pipeline, ensuring it's optimized for Intel GPUs.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be12d985-31b5-4a03-b976-74787558fc0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Text2ImgModel:\n",
    "    \"\"\"\n",
    "    Text2ImgModel is a class for generating images based on text prompts using a pretrained model.\n",
    "\n",
    "    Attributes:\n",
    "    - device: The device to run the model on. Default to \"xpu\" - Intel dGPUs.\n",
    "    - pipeline: The loaded model pipeline.\n",
    "    - data_type: The data type to use in the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id_or_path: str,\n",
    "        device: str = \"xpu\",\n",
    "        torch_dtype: torch.dtype = torch.bfloat16,\n",
    "        optimize: bool = True,\n",
    "        enable_scheduler: bool = False,\n",
    "        warmup: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        The initializer for Text2ImgModel class.\n",
    "\n",
    "        Parameters:\n",
    "        - model_id_or_path: The identifier or path of the pretrained model.\n",
    "        - device: The device to run the model on. Default is \"xpu\".\n",
    "        - torch_dtype: The data type to use in the model. Default is torch.bfloat16.\n",
    "        - optimize: Whether to optimize the model after loading. Default is True.\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = device\n",
    "        self.pipeline = self._load_pipeline(\n",
    "            model_id_or_path, torch_dtype, enable_scheduler\n",
    "        )\n",
    "        self.data_type = torch_dtype\n",
    "        if optimize:\n",
    "            start_time = time.time()\n",
    "            #print(\"Optimizing the model...\")\n",
    "            self.optimize_pipeline()\n",
    "            #print(\n",
    "            #    \"Optimization completed in {:.2f} seconds.\".format(\n",
    "            #        time.time() - start_time\n",
    "            #    )\n",
    "            #)\n",
    "        if warmup:\n",
    "            self.warmup_model()\n",
    "\n",
    "    def _load_pipeline(\n",
    "        self,\n",
    "        model_id_or_path: str,\n",
    "        torch_dtype: torch.dtype,\n",
    "        enable_scheduler: bool,\n",
    "\n",
    "    ) -> DiffusionPipeline:\n",
    "        \"\"\"\n",
    "        Loads the pretrained model and prepares it for inference.\n",
    "\n",
    "        Parameters:\n",
    "        - model_id_or_path: The identifier or path of the pretrained model.\n",
    "        - torch_dtype: The data type to use in the model.\n",
    "\n",
    "        Returns:\n",
    "        - pipeline: The loaded model pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Loading the model...\")\n",
    "        model_path = Path(f\"/home/common/data/Big_Data/GenAI/{model_id_or_path}\")  \n",
    "        \n",
    "        if model_path.exists():\n",
    "            #print(f\"Loading the model from {model_path}...\")\n",
    "            load_path = model_path\n",
    "        else:\n",
    "            print(\"Using the default path for models...\")\n",
    "            load_path = model_id_or_path\n",
    "\n",
    "        pipeline = DiffusionPipeline.from_pretrained(\n",
    "            load_path,\n",
    "            torch_dtype=torch_dtype,\n",
    "            use_safetensors=True,\n",
    "            variant=\"fp16\",\n",
    "        )\n",
    "        if enable_scheduler:\n",
    "            pipeline.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "                pipeline.scheduler.config\n",
    "            )\n",
    "        if not model_path.exists():\n",
    "            try:\n",
    "                print(f\"Attempting to save the model to {model_path}...\")\n",
    "                pipeline.save_pretrained(f\"{model_path}\")\n",
    "                print(\"Model saved.\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while saving the model: {e}. Proceeding without saving.\")\n",
    "        pipeline = pipeline.to(self.device)\n",
    "        #print(\"Model loaded.\")\n",
    "        return pipeline\n",
    "\n",
    "    def _optimize_pipeline(self, pipeline: DiffusionPipeline) -> DiffusionPipeline:\n",
    "        \"\"\"\n",
    "        Optimizes the model for inference using ipex.\n",
    "\n",
    "        Parameters:\n",
    "        - pipeline: The model pipeline to be optimized.\n",
    "\n",
    "        Returns:\n",
    "        - pipeline: The optimized model pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        for attr in dir(pipeline):\n",
    "            if isinstance(getattr(pipeline, attr), nn.Module):\n",
    "                setattr(\n",
    "                    pipeline,\n",
    "                    attr,\n",
    "                    ipex.optimize(\n",
    "                        getattr(pipeline, attr).eval(),\n",
    "                        dtype=pipeline.text_encoder.dtype,\n",
    "                        inplace=True,\n",
    "                    ),\n",
    "                )\n",
    "        return pipeline\n",
    "\n",
    "    def warmup_model(self):\n",
    "        \"\"\"\n",
    "        Warms up the model by generating a sample image.\n",
    "        \"\"\"\n",
    "        print(\"Setting up model...\")\n",
    "        start_time = time.time()\n",
    "        self.generate_images(\n",
    "            prompt=\"A beautiful sunset over the mountains\",\n",
    "            num_images=1,\n",
    "            save_path=\".tmp\",\n",
    "        )\n",
    "        print(\n",
    "            \"Model is set up and ready! Warm-up completed in {:.2f} seconds.\".format(\n",
    "                time.time() - start_time\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def optimize_pipeline(self) -> None:\n",
    "        \"\"\"\n",
    "        Optimizes the current model pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        self.pipeline = self._optimize_pipeline(self.pipeline)\n",
    "\n",
    "    def generate_images(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        frame_num:int,\n",
    "        num_inference_steps: int = 50,\n",
    "        num_images: int = 5,\n",
    "        save_path: str = \"output\"\n",
    "    ) -> List[Image.Image]:\n",
    "        \"\"\"\n",
    "        Generates images based on the given prompt and saves them to disk.\n",
    "\n",
    "        Parameters:\n",
    "        - prompt: The text prompt to generate images from.\n",
    "        - num_inference_steps: Number of noise removal steps.\n",
    "        - num_images: The number of images to generate. Default is 5.\n",
    "        - save_path: The directory to save the generated images in. Default is \"output\".\n",
    "\n",
    "        Returns:\n",
    "        - images: A list of the generated images.\n",
    "        \"\"\"\n",
    "\n",
    "        images = []\n",
    "        for i in range(num_images):\n",
    "            with torch.xpu.amp.autocast(\n",
    "                enabled=True if self.data_type != torch.float32 else False,\n",
    "                dtype=self.data_type,\n",
    "            ):\n",
    "                image = self.pipeline(\n",
    "                    prompt=prompt,\n",
    "                    num_inference_steps=num_inference_steps,\n",
    "                    #negative_prompt=negative_prompt,\n",
    "                ).images[0]\n",
    "                if not os.path.exists(save_path):\n",
    "                    try:\n",
    "                        os.makedirs(save_path)\n",
    "                    except OSError as e:\n",
    "                        print(\"Failed to create directory\", save_path, \"due to\", str(e))\n",
    "                        raise\n",
    "            output_image_path = os.path.join(\n",
    "                save_path,\n",
    "                f\"{frame_num}_{i}_{sum(ord(c) for c in prompt) % 10000}.png\",\n",
    "            )\n",
    "            image.save(output_image_path)\n",
    "            images.append(image)\n",
    "        return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bbd62b-02d1-4316-96af-0a54fc970c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mp_img\n",
    "\n",
    "# from IPython.display import Image as IPImage\n",
    "\n",
    "def display_generated_images(output_dir=\"output\"):\n",
    "    image_files = [f for f in os.listdir(output_dir) if f.endswith((\".png\", \".jpg\"))]    \n",
    "    num_images = len(image_files)\n",
    "    num_columns = int(np.ceil(np.sqrt(num_images)))\n",
    "    num_rows = int(np.ceil(num_images / num_columns))\n",
    "    fig, axs = plt.subplots(num_rows, num_columns, figsize=(10 * num_columns / num_columns, 10 * num_rows / num_rows))\n",
    "    if num_images == 1:\n",
    "        axs = np.array([[axs]])\n",
    "    elif num_columns == 1 or num_rows == 1:\n",
    "        axs = np.array([axs])\n",
    "    for ax, image_file in zip(axs.ravel(), image_files):\n",
    "        img = mp_img.imread(os.path.join(output_dir, image_file))\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")  # Hide axes\n",
    "    for ax in axs.ravel()[num_images:]:\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    print(f\"\\nGenerated images...:\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a22cebb1-2583-4a26-afd8-e951ff8d1a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model...\n",
      "Using the default path for models...\n",
      "Attempting to save the model to /home/common/data/Big_Data/GenAI/stabilityai/stable-diffusion-2-1...\n",
      "An error occurred while saving the model: [Errno 45] Operation not supported: '/home/common'. Proceeding without saving.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_id \u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstabilityai/stable-diffusion-2-1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# model=Text2ImgModel(model_id, device=\"xpu\")\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m=\u001b[39mText2ImgModel(model_id, device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mThe initializer for Text2ImgModel class.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m- optimize: Whether to optimize the model after loading. Default is True.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m device\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pipeline(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     model_id_or_path, torch_dtype, enable_scheduler\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_type \u001b[39m=\u001b[39m torch_dtype\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mif\u001b[39;00m optimize:\n",
      "\u001b[1;32m/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb Cell 10\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while saving the model: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m. Proceeding without saving.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m pipeline \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39m#print(\"Model loaded.\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tzvifriedman/dev/intel-genai-hackathon-dec-2023/Challenge1.ipynb#X12sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pipeline\n",
      "File \u001b[0;32m~/dev/intel-genai-hackathon-dec-2023/.conda/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:681\u001b[0m, in \u001b[0;36mDiffusionPipeline.to\u001b[0;34m(self, torch_device, torch_dtype, silence_dtype_warnings)\u001b[0m\n\u001b[1;32m    677\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    678\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe module \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has been loaded in 8bit and moving it to \u001b[39m\u001b[39m{\u001b[39;00mtorch_dtype\u001b[39m}\u001b[39;00m\u001b[39m via `.to()` is not yet supported. Module is still on \u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    680\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 681\u001b[0m     module\u001b[39m.\u001b[39;49mto(torch_device, torch_dtype)\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    684\u001b[0m     module\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16\n\u001b[1;32m    685\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mstr\u001b[39m(torch_device) \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    686\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m silence_dtype_warnings\n\u001b[1;32m    687\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_offloaded\n\u001b[1;32m    688\u001b[0m ):\n\u001b[1;32m    689\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    690\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPipelines loaded with `torch_dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    691\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    695\u001b[0m     )\n",
      "File \u001b[0;32m~/dev/intel-genai-hackathon-dec-2023/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1141\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1055\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves and/or casts the parameters and buffers.\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \n\u001b[1;32m   1057\u001b[0m \u001b[39m    This can be called as\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \n\u001b[1;32m   1139\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1141\u001b[0m     device, dtype, non_blocking, convert_to_format \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49m_parse_to(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1144\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (dtype\u001b[39m.\u001b[39mis_floating_point \u001b[39mor\u001b[39;00m dtype\u001b[39m.\u001b[39mis_complex):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu"
     ]
    }
   ],
   "source": [
    "model_id =\"stabilityai/stable-diffusion-2-1\"\n",
    "model=Text2ImgModel(model_id, device=\"xpu\")\n",
    "# model=Text2ImgModel(model_id, device=\"gpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0659855b-9f18-42c5-8553-a07449ce4e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a6b51d603544b58624b827b2c8f588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1adda063ed743a7aab6da6949fe45ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21de80800dbe430fabd26281e2277067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aedb818b6ad47dcaff8631a555184de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c6ea364f204f59af0651dd67af95dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2c77f7947343aba5e1a1b3f4eafba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684ce664c9424bc9842ede23daae6ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e3651ce5ba43a4a0448fda0dd292dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f46e0e52de8461989c3265835eb09db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90425b4605a4fd2b6edeaa933ff3361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b2e42ccddf490d9f8c2eb908c6c36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d546e3c30c9e41ddae99d2088bfd93e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)\n",
    "\n",
    "num_images=3\n",
    "enhancements= \"Cartoon, tranquil, art\"\n",
    "subject = \"\"\"Create an image showing Carol, a retired school teacher, who had always been passionate about gardening. She has red hair, is wearing dark brown pants, a forest green t-shirt, and has sneakers.\"\"\" \n",
    "actions = [\n",
    "\"\"\"She is standing in front of a bare plot of a land at her local park which is bare and underutilized.\"\"\",\n",
    "\"\"\"She started by planting a few flower beds and a small vegetable patch. Carol worked tirelessly, often spending her mornings tending to the garden\"\"\",\n",
    "\"\"\"Show Carol surrounded by neighbors who are intriguided and inspired to help her by sharing seeds and planting.\"\"\",\n",
    "\"\"\"Show Carol with school children at the garden, teaching them about sustainability\"\"\",\n",
    "\"\"\"Show Carol standing proudly next to her garden which is now a centerpiece of the community, recognized by the community council\"\"\"\n",
    "]\n",
    "for i,action in enumerate(actions):\n",
    "    model.generate_images(\n",
    "        subject + action + enhancements,\n",
    "        num_images=num_images,\n",
    "        frame_num=i,\n",
    "        save_path=\"./output\",\n",
    "    )\n",
    "display_generated_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c760991-214d-4990-8153-25b5dc032618",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!--#### Reference and Guidelines for Models Used in This Notebook\n",
    "\n",
    "##### CompVis/stable-diffusion-v1-4\n",
    "- **Model card:** [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4)\n",
    "- **License:** CreativeML OpenRAIL M license\n",
    "- **Reference:**\n",
    "    ```bibtex\n",
    "    @InProceedings{Rombach_2022_CVPR,\n",
    "        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n",
    "        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n",
    "        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "        month     = {June},\n",
    "        year      = {2022},\n",
    "        pages     = {10684-10695}\n",
    "    }\n",
    "    ```\n",
    "\n",
    "##### stabilityai/stable-diffusion-2\n",
    "- **Model card:** [stabilityai/stable-diffusion-2](https://huggingface.co/stabilityai/stable-diffusion-2)\n",
    "- **License:** CreativeML Open RAIL++-M License\n",
    "- **Reference:**\n",
    "    ```bibtex\n",
    "    @InProceedings{Rombach_2022_CVPR,\n",
    "        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n",
    "        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n",
    "        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "        month     = {June},\n",
    "        year      = {2022},\n",
    "        pages     = {10684-10695}\n",
    "    }\n",
    "    ```\n",
    "\n",
    "##### Disclaimer for Using Stable Diffusion Models\n",
    "\n",
    "The stable diffusion models provided here are powerful tools for high-resolution image synthesis, including text-to-image and image-to-image transformations. While they are designed to produce high-quality results, users should be aware of potential limitations:\n",
    "\n",
    "- **Quality Variation:** The quality of generated images may vary based on the complexity of the input text or image, and the alignment with the model's training data.\n",
    "- **Licensing and Usage Constraints:** Please carefully review the licensing information associated with each model to ensure compliance with all terms and conditions.\n",
    "- **Ethical Considerations:** Consider the ethical implications of the generated content, especially in contexts that may involve sensitive or controversial subjects.\n",
    "\n",
    "For detailed information on each model's capabilities, limitations, and best practices, please refer to the respective model cards and associated publications linked below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c16a9-7186-4535-9e77-d67cefbdac35",
   "metadata": {},
   "source": [
    "#### Reference and Guidelines for Models Used in This Notebook\n",
    "\n",
    "##### stabilityai/stable-diffusion-2\n",
    "- **Model card:** [stabilityai/stable-diffusion-2](https://huggingface.co/stabilityai/stable-diffusion-2)\n",
    "- **License:** CreativeML Open RAIL++-M License\n",
    "- **Reference:**\n",
    "    ```bibtex\n",
    "    @InProceedings{Rombach_2022_CVPR,\n",
    "        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n",
    "        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n",
    "        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "        month     = {June},\n",
    "        year      = {2022},\n",
    "        pages     = {10684-10695}\n",
    "    }\n",
    "    ```\n",
    "\n",
    "#### Disclaimer for Using Stable Diffusion Models\n",
    "\n",
    "The stable diffusion models provided here are powerful tools for high-resolution image synthesis, including text-to-image and image-to-image transformations. While they are designed to produce high-quality results, users should be aware of potential limitations:\n",
    "\n",
    "- **Quality Variation:** The quality of generated images may vary based on the complexity of the input text or image, and the alignment with the model's training data.\n",
    "- **Licensing and Usage Constraints:** Please carefully review the licensing information associated with each model to ensure compliance with all terms and conditions.\n",
    "- **Ethical Considerations:** Consider the ethical implications of the generated content, especially in contexts that may involve sensitive or controversial subjects.\n",
    "\n",
    "For detailed information on each model's capabilities, limitations, and best practices, please refer to the respective model cards and associated publications linked below."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
